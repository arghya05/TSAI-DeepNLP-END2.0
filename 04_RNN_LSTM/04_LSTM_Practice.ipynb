{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_LSTM_Practice.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "edhClIWVO0km"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyajitghana/TSAI-DeepNLP-END2.0/blob/main/04_RNN_LSTM/04_LSTM_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jofyc9OC4Qcf"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahBVnrNc3E0U"
      },
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "plt.style.use('seaborn-white')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crQSAaIz4SkA"
      },
      "source": [
        "# Read and process data. \n",
        "\n",
        "Download the file from this URL: https://drive.google.com/file/d/1UWWIi-sz9g0x3LFvkIZjvK1r2ZaCqgGS/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WYQWjTLak6V",
        "outputId": "c5637c14-a1bf-4390-acea-fb20980fb38f"
      },
      "source": [
        "! gdown https://drive.google.com/uc?id=1UWWIi-sz9g0x3LFvkIZjvK1r2ZaCqgGS"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UWWIi-sz9g0x3LFvkIZjvK1r2ZaCqgGS\n",
            "To: /content/text.txt\n",
            "\r  0% 0.00/10.3k [00:00<?, ?B/s]\r100% 10.3k/10.3k [00:00<00:00, 18.9MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgOGxPDP3Wpp"
      },
      "source": [
        "data = open('text.txt', 'r').read()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeXXMLRb4kXb"
      },
      "source": [
        "Process data and calculate indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5TKeiOp4jtl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93d2fa59-2b63-46ec-87d6-1317b182251e"
      },
      "source": [
        "chars = list(set(data))\n",
        "data_size, X_size = len(data), len(chars)\n",
        "print(\"Corona Virus article has %d characters, %d unique characters\" %(data_size, X_size))\n",
        "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
        "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corona Virus article has 10223 characters, 75 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C53MB135LRY"
      },
      "source": [
        "# Constants and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfj21ORa49Ps"
      },
      "source": [
        "Hidden_Layer_size = 10 #size of the hidden layer\n",
        "Time_steps = 10 # Number of time steps (length of the sequence) used for training\n",
        "learning_rate = 1e-1 # Learning Rate\n",
        "weight_sd = 0.1 #Standard deviation of weights for initialization\n",
        "z_size = Hidden_Layer_size + X_size #Size of concatenation(H, X) vector"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdmJf4Du5uhb"
      },
      "source": [
        "# Activation Functions and Derivatives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seGHei_D5FGk"
      },
      "source": [
        "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def dsigmoid(y): return y * (1 - y)\n",
        "\n",
        "def tanh(x): return np.tanh(x)\n",
        "\n",
        "def dtanh(y): return 1 - y * y"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE5sBCokMQPk"
      },
      "source": [
        "# Quiz Question 1\n",
        "\n",
        "What is the value of sigmoid(0) calculated from  your code? (Answer up to 1 decimal point, e.g. 4.2 and NOT 4.29999999, no rounding off)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DECYTOdoMPeW",
        "outputId": "5dcc671c-f9db-4aff-e704-32e97e3944fb"
      },
      "source": [
        "sigmoid(0)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldaF0o7mMbS1"
      },
      "source": [
        "# Quiz Question 2\n",
        "\n",
        "What is the value of dsigmoid(sigmoid(0)) calculated from your code?? (Answer up to 2 decimal point, e.g. 4.29 and NOT 4.29999999, no rounding off). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1i37ID6MdDo",
        "outputId": "c100c700-d11f-4010-eab6-646598937404"
      },
      "source": [
        "dsigmoid(sigmoid(0))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78D9IUC5MhTn"
      },
      "source": [
        "# Quiz Question 3\n",
        "\n",
        "What is the value of tanh(dsigmoid(sigmoid(0))) calculated from your code?? (Answer up to 5 decimal point, e.g. 4.29999 and NOT 4.29999999, no rounding off)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwwrPTopMi7M",
        "outputId": "cc2bdae4-79a8-4c24-f02a-75633818e127"
      },
      "source": [
        "tanh(dsigmoid(sigmoid(0)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.24491866240370913"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeCvVH1v6Me-"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Quiz Question 4\n",
        "\n",
        "What is the value of dtanh(tanh(dsigmoid(sigmoid(0)))) calculated from your code?? (Answer up to 5 decimal point, e.g. 4.29999 and NOT 4.29999999, no rounding off)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8eRxHzPMoBr",
        "outputId": "dc84b093-aef6-48c8-fe3e-b550f922b7be"
      },
      "source": [
        "dtanh(tanh(dsigmoid(sigmoid(0))))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.940014848806378"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeSVipDu8iKE"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICbWNemE6LGV"
      },
      "source": [
        "class Param:\n",
        "    def __init__(self, name, value):\n",
        "      self.name = name\n",
        "      self.v = value # parameter value\n",
        "      self.d = np.zeros_like(value) # derivative\n",
        "      self.m = np.zeros_like(value) # momentum for Adagrad"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j83pZNPE8212"
      },
      "source": [
        "We use random weights with normal distribution (0, weight_sd) for  tanh  activation function and (0.5, weight_sd) for  `sigmoid`  activation function.\n",
        "\n",
        "Biases are initialized to zeros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swHwLXOI9E7V"
      },
      "source": [
        "# LSTM \n",
        "You are making this network, please note f, i, c and o (also \"v\") in the image below:\n",
        "![alt text](http://blog.varunajayasiri.com/ml/lstm.svg)\n",
        "\n",
        "Please note that we are concatenating the old_hidden_vector and new_input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0DBzNY-90s5"
      },
      "source": [
        "# Quiz Question 5\n",
        "\n",
        "In the class definition below, what should be size_a, size_b, and size_c? ONLY use the variables defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFuHhqVq6Wge"
      },
      "source": [
        "size_a = Hidden_Layer_size # write your code here\n",
        "size_b = z_size # write your code here\n",
        "size_c = X_size # write your code here\n",
        "\n",
        "class Parameters:\n",
        "    def __init__(self):\n",
        "        self.W_f = Param('W_f', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_f = Param('b_f', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_i = Param('W_i', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_i = Param('b_i', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_C = Param('W_C', np.random.randn(size_a, size_b) * weight_sd)\n",
        "        self.b_C = Param('b_C', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_o = Param('W_o', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_o = Param('b_o', np.zeros((size_a, 1)))\n",
        "\n",
        "        #For final layer to predict the next character\n",
        "        self.W_v = Param('W_v', np.random.randn(X_size, size_a) * weight_sd)\n",
        "        self.b_v = Param('b_v', np.zeros((size_c, 1)))\n",
        "        \n",
        "    def all(self):\n",
        "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
        "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
        "        \n",
        "parameters = Parameters()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzmfGLZt_xVs"
      },
      "source": [
        "Look at these operations which we'll be writing:\n",
        "\n",
        "**Concatenation of h and x:**\n",
        "\n",
        "$z\\:=\\:\\left[h_{t-1},\\:x\\right]$\n",
        "\n",
        "$f_t=\\sigma\\left(W_f\\cdot z\\:+\\:b_f\\:\\right)$\n",
        "\n",
        "$i_i=\\sigma\\left(W_i\\cdot z\\:+\\:b_i\\right)$\n",
        "\n",
        "$\\overline{C_t}=\\tanh\\left(W_C\\cdot z\\:+\\:b_C\\right)$\n",
        "\n",
        "$C_t=f_t\\ast C_{t-1}+i_t\\ast \\overline{C}_t$\n",
        "\n",
        "$o_t=\\sigma\\left(W_o\\cdot z\\:+\\:b_o\\right)$\n",
        "\n",
        "$h_t=o_t\\ast\\tanh\\left(C_t\\right)$\n",
        "\n",
        "**Logits:**\n",
        "\n",
        "$v_t=W_v\\cdot h_t+b_v$\n",
        "\n",
        "**Softmax:**\n",
        "\n",
        "$\\hat{y}=softmax\\left(v_t\\right)$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bUkseNnDott"
      },
      "source": [
        "def forward(x, h_prev, C_prev, p = parameters):\n",
        "    assert x.shape == (X_size, 1)\n",
        "    assert h_prev.shape == (Hidden_Layer_size, 1)\n",
        "    assert C_prev.shape == (Hidden_Layer_size, 1)\n",
        "    \n",
        "    z = np.row_stack((h_prev, x))\n",
        "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
        "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
        "    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
        "\n",
        "    C = f * C_prev + i * C_bar\n",
        "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
        "    h = o * tanh(C)\n",
        "\n",
        "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
        "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
        "\n",
        "    return z, f, i, C_bar, C, o, h, v, y"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZrDhZIjFpdI"
      },
      "source": [
        "You must finish the function above before you can attempt the questions below. \n",
        "\n",
        "# Quiz Question 6\n",
        "\n",
        "What is the output of 'print(len(forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)), parameters)))'?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmKU4RP8M53y",
        "outputId": "c0511e03-5748-4e61-d7dd-a891b82210e2"
      },
      "source": [
        "print(len(forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)), parameters)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV-YVl_GGiX8"
      },
      "source": [
        "# Quiz Question 7 \n",
        "\n",
        "Assuming you have fixed the forward function, run this command: \n",
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))\n",
        "\n",
        "Now, find these values:\n",
        "\n",
        "\n",
        "1.   print(z.shape)\n",
        "2.   print(np.sum(z))\n",
        "3.   print(np.sum(f))\n",
        "\n",
        "Copy and paste exact values you get in the logs into the quiz.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GvKVWmTDt3H"
      },
      "source": [
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8pdZv9_NBPl",
        "outputId": "8b258719-3da1-4c3c-c0f5-a7348b347c1b"
      },
      "source": [
        "print(z.shape)\n",
        "print(np.sum(z))\n",
        "print(np.sum(f))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(85, 1)\n",
            "0.0\n",
            "5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeSvhkqwILsG"
      },
      "source": [
        "# Backpropagation\n",
        "\n",
        "Here we are defining the backpropagation. It's too complicated, here is the whole code. (Please note that this would work only if your earlier code is perfect)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIa1jUZiGPmF"
      },
      "source": [
        "def backward(target, dh_next, dC_next, C_prev,\n",
        "             z, f, i, C_bar, C, o, h, v, y,\n",
        "             p = parameters):\n",
        "    \n",
        "    assert z.shape == (X_size + Hidden_Layer_size, 1)\n",
        "    assert v.shape == (X_size, 1)\n",
        "    assert y.shape == (X_size, 1)\n",
        "    \n",
        "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
        "        assert param.shape == (Hidden_Layer_size, 1)\n",
        "        \n",
        "    dv = np.copy(y)\n",
        "    dv[target] -= 1\n",
        "\n",
        "    p.W_v.d += np.dot(dv, h.T)\n",
        "    p.b_v.d += dv\n",
        "\n",
        "    dh = np.dot(p.W_v.v.T, dv)        \n",
        "    dh += dh_next\n",
        "    do = dh * tanh(C)\n",
        "    do = dsigmoid(o) * do\n",
        "    p.W_o.d += np.dot(do, z.T)\n",
        "    p.b_o.d += do\n",
        "\n",
        "    dC = np.copy(dC_next)\n",
        "    dC += dh * o * dtanh(tanh(C))\n",
        "    dC_bar = dC * i\n",
        "    dC_bar = dtanh(C_bar) * dC_bar\n",
        "    p.W_C.d += np.dot(dC_bar, z.T)\n",
        "    p.b_C.d += dC_bar\n",
        "\n",
        "    di = dC * C_bar\n",
        "    di = dsigmoid(i) * di\n",
        "    p.W_i.d += np.dot(di, z.T)\n",
        "    p.b_i.d += di\n",
        "\n",
        "    df = dC * C_prev\n",
        "    df = dsigmoid(f) * df\n",
        "    p.W_f.d += np.dot(df, z.T)\n",
        "    p.b_f.d += df\n",
        "\n",
        "    dz = (np.dot(p.W_f.v.T, df)\n",
        "         + np.dot(p.W_i.v.T, di)\n",
        "         + np.dot(p.W_C.v.T, dC_bar)\n",
        "         + np.dot(p.W_o.v.T, do))\n",
        "    dh_prev = dz[:Hidden_Layer_size, :]\n",
        "    dC_prev = f * dC\n",
        "    \n",
        "    return dh_prev, dC_prev"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnc7WpRkIU5S"
      },
      "source": [
        "# Forward and Backward Combined Pass\n",
        "\n",
        "Let's first clear the gradients before each backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJWoC3U1ITf8"
      },
      "source": [
        "def clear_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.d.fill(0)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XN93UnjIgmA"
      },
      "source": [
        "Clip gradients to mitigate exploding gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LTsublxIfFl"
      },
      "source": [
        "def clip_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        np.clip(p.d, -1, 1, out=p.d)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7XUpDTWIl_Y"
      },
      "source": [
        "Calculate and store the values in forward pass. Accumulate gradients in backward pass and clip gradients to avoid exploding gradients.\n",
        "\n",
        "input, target are list of integers, with character indexes.\n",
        "h_prev is the array of initial h at  h−1  (size H x 1)\n",
        "C_prev is the array of initial C at  C−1  (size H x 1)\n",
        "Returns loss, final  hT  and  CT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQNxjTuZIia_"
      },
      "source": [
        "def forward_backward(inputs, targets, h_prev, C_prev):\n",
        "    global paramters\n",
        "    \n",
        "    # To store the values for each time step\n",
        "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
        "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
        "    v_s, y_s =  {}, {}\n",
        "    \n",
        "    # Values at t - 1\n",
        "    h_s[-1] = np.copy(h_prev)\n",
        "    C_s[-1] = np.copy(C_prev)\n",
        "    \n",
        "    loss = 0\n",
        "    # Loop through time steps\n",
        "    assert len(inputs) == Time_steps\n",
        "    for t in range(len(inputs)):\n",
        "        x_s[t] = np.zeros((X_size, 1))\n",
        "        x_s[t][inputs[t]] = 1 # Input character\n",
        "        \n",
        "        (z_s[t], f_s[t], i_s[t],\n",
        "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
        "        v_s[t], y_s[t]) = \\\n",
        "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
        "            \n",
        "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
        "        \n",
        "    clear_gradients()\n",
        "\n",
        "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
        "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        # Backward pass\n",
        "        dh_next, dC_next = \\\n",
        "            backward(target = targets[t], dh_next = dh_next,\n",
        "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
        "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
        "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
        "                     y = y_s[t])\n",
        "\n",
        "    clip_gradients()\n",
        "        \n",
        "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcy5u_vRItkV"
      },
      "source": [
        "# Sample the next character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8SrtJiwIsSm"
      },
      "source": [
        "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
        "    x = np.zeros((X_size, 1))\n",
        "    x[first_char_idx] = 1\n",
        "\n",
        "    h = h_prev\n",
        "    C = C_prev\n",
        "\n",
        "    indexes = []\n",
        "    \n",
        "    for t in range(sentence_length):\n",
        "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
        "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
        "        x = np.zeros((X_size, 1))\n",
        "        x[idx] = 1\n",
        "        indexes.append(idx)\n",
        "\n",
        "    return indexes"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiWFaWLNIx_L"
      },
      "source": [
        "# Training (Adagrad)\n",
        "\n",
        "Update the graph and display a sample output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENQYU-7AIw0t"
      },
      "source": [
        "def update_status(inputs, h_prev, C_prev):\n",
        "    #initialized later\n",
        "    global plot_iter, plot_loss\n",
        "    global smooth_loss\n",
        "    \n",
        "    # Get predictions for 200 letters with current model\n",
        "\n",
        "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
        "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
        "\n",
        "    # Clear and plot\n",
        "    plt.plot(plot_iter, plot_loss)\n",
        "    display.clear_output(wait=True)\n",
        "    plt.show()\n",
        "\n",
        "    #Print prediction and loss\n",
        "    print(\"----\\n %s \\n----\" % (txt, ))\n",
        "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACXcASJuI73a"
      },
      "source": [
        "# Update Parameters\n",
        "\n",
        "\\begin{align}\n",
        "\\theta_i &= \\theta_i - \\eta\\frac{d\\theta_i}{\\sum dw_{\\tau}^2} \\\\\n",
        "d\\theta_i &= \\frac{\\partial L}{\\partial \\theta_i}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR08TvcjI4Pf"
      },
      "source": [
        "def update_paramters(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.m += p.d * p.d # Calculate sum of gradients\n",
        "        #print(learning_rate * dparam)\n",
        "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La9vyJ6RJLFK"
      },
      "source": [
        "To delay the keyboard interrupt to prevent the training from stopping in the middle of an iteration\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVDHbMb7JNGT"
      },
      "source": [
        "# Exponential average of loss\n",
        "# Initialize to a error of a random model\n",
        "smooth_loss = -np.log(1.0 / X_size) * Time_steps\n",
        "\n",
        "iteration, pointer = 0, 0\n",
        "\n",
        "# For the graph\n",
        "plot_iter = np.zeros((0))\n",
        "plot_loss = np.zeros((0))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF6vS0VWJqsS"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edhClIWVO0km"
      },
      "source": [
        "## All the previous cells"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8owA9Ba0O_Z7"
      },
      "source": [
        "Hidden_Layer_size = 100 #size of the hidden layer\n",
        "Time_steps = 40 # Number of time steps (length of the sequence) used for training\n",
        "learning_rate = 1e-1 # Learning Rate\n",
        "weight_sd = 0.1 #Standard deviation of weights for initialization\n",
        "z_size = Hidden_Layer_size + X_size #Size of concatenation(H, X) vector"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdGBImuJO_aE"
      },
      "source": [
        "class Param:\n",
        "    def __init__(self, name, value):\n",
        "      self.name = name\n",
        "      self.v = value # parameter value\n",
        "      self.d = np.zeros_like(value) # derivative\n",
        "      self.m = np.zeros_like(value) # momentum for Adagrad"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS3jVfjjO_aF"
      },
      "source": [
        "size_a = Hidden_Layer_size # write your code here\n",
        "size_b = z_size # write your code here\n",
        "size_c = X_size # write your code here\n",
        "\n",
        "class Parameters:\n",
        "    def __init__(self):\n",
        "        self.W_f = Param('W_f', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_f = Param('b_f', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_i = Param('W_i', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_i = Param('b_i', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_C = Param('W_C', np.random.randn(size_a, size_b) * weight_sd)\n",
        "        self.b_C = Param('b_C', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_o = Param('W_o', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_o = Param('b_o', np.zeros((size_a, 1)))\n",
        "\n",
        "        #For final layer to predict the next character\n",
        "        self.W_v = Param('W_v', np.random.randn(X_size, size_a) * weight_sd)\n",
        "        self.b_v = Param('b_v', np.zeros((size_c, 1)))\n",
        "        \n",
        "    def all(self):\n",
        "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
        "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
        "        \n",
        "parameters = Parameters()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl0p3Hi0O_aG"
      },
      "source": [
        "def forward(x, h_prev, C_prev, p = parameters):\n",
        "    assert x.shape == (X_size, 1)\n",
        "    assert h_prev.shape == (Hidden_Layer_size, 1)\n",
        "    assert C_prev.shape == (Hidden_Layer_size, 1)\n",
        "    \n",
        "    z = np.row_stack((h_prev, x))\n",
        "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
        "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
        "    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
        "\n",
        "    C = f * C_prev + i * C_bar\n",
        "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
        "    h = o * tanh(C)\n",
        "\n",
        "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
        "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
        "\n",
        "    return z, f, i, C_bar, C, o, h, v, y"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAqCVuAOO_aH"
      },
      "source": [
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zysgmuXaO_aH"
      },
      "source": [
        "def backward(target, dh_next, dC_next, C_prev,\n",
        "             z, f, i, C_bar, C, o, h, v, y,\n",
        "             p = parameters):\n",
        "    \n",
        "    assert z.shape == (X_size + Hidden_Layer_size, 1)\n",
        "    assert v.shape == (X_size, 1)\n",
        "    assert y.shape == (X_size, 1)\n",
        "    \n",
        "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
        "        assert param.shape == (Hidden_Layer_size, 1)\n",
        "        \n",
        "    dv = np.copy(y)\n",
        "    dv[target] -= 1\n",
        "\n",
        "    p.W_v.d += np.dot(dv, h.T)\n",
        "    p.b_v.d += dv\n",
        "\n",
        "    dh = np.dot(p.W_v.v.T, dv)        \n",
        "    dh += dh_next\n",
        "    do = dh * tanh(C)\n",
        "    do = dsigmoid(o) * do\n",
        "    p.W_o.d += np.dot(do, z.T)\n",
        "    p.b_o.d += do\n",
        "\n",
        "    dC = np.copy(dC_next)\n",
        "    dC += dh * o * dtanh(tanh(C))\n",
        "    dC_bar = dC * i\n",
        "    dC_bar = dtanh(C_bar) * dC_bar\n",
        "    p.W_C.d += np.dot(dC_bar, z.T)\n",
        "    p.b_C.d += dC_bar\n",
        "\n",
        "    di = dC * C_bar\n",
        "    di = dsigmoid(i) * di\n",
        "    p.W_i.d += np.dot(di, z.T)\n",
        "    p.b_i.d += di\n",
        "\n",
        "    df = dC * C_prev\n",
        "    df = dsigmoid(f) * df\n",
        "    p.W_f.d += np.dot(df, z.T)\n",
        "    p.b_f.d += df\n",
        "\n",
        "    dz = (np.dot(p.W_f.v.T, df)\n",
        "         + np.dot(p.W_i.v.T, di)\n",
        "         + np.dot(p.W_C.v.T, dC_bar)\n",
        "         + np.dot(p.W_o.v.T, do))\n",
        "    dh_prev = dz[:Hidden_Layer_size, :]\n",
        "    dC_prev = f * dC\n",
        "    \n",
        "    return dh_prev, dC_prev"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeBM6dyVO_aI"
      },
      "source": [
        "def clear_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.d.fill(0)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRE1ycA0O_aI"
      },
      "source": [
        "def clip_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        np.clip(p.d, -1, 1, out=p.d)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsHpPe3gO_aJ"
      },
      "source": [
        "def forward_backward(inputs, targets, h_prev, C_prev):\n",
        "    global paramters\n",
        "    \n",
        "    # To store the values for each time step\n",
        "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
        "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
        "    v_s, y_s =  {}, {}\n",
        "    \n",
        "    # Values at t - 1\n",
        "    h_s[-1] = np.copy(h_prev)\n",
        "    C_s[-1] = np.copy(C_prev)\n",
        "    \n",
        "    loss = 0\n",
        "    # Loop through time steps\n",
        "    assert len(inputs) == Time_steps\n",
        "    for t in range(len(inputs)):\n",
        "        x_s[t] = np.zeros((X_size, 1))\n",
        "        x_s[t][inputs[t]] = 1 # Input character\n",
        "        \n",
        "        (z_s[t], f_s[t], i_s[t],\n",
        "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
        "        v_s[t], y_s[t]) = \\\n",
        "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
        "            \n",
        "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
        "        \n",
        "    clear_gradients()\n",
        "\n",
        "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
        "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        # Backward pass\n",
        "        dh_next, dC_next = \\\n",
        "            backward(target = targets[t], dh_next = dh_next,\n",
        "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
        "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
        "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
        "                     y = y_s[t])\n",
        "\n",
        "    clip_gradients()\n",
        "        \n",
        "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L368yJm7O_aK"
      },
      "source": [
        "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
        "    x = np.zeros((X_size, 1))\n",
        "    x[first_char_idx] = 1\n",
        "\n",
        "    h = h_prev\n",
        "    C = C_prev\n",
        "\n",
        "    indexes = []\n",
        "    \n",
        "    for t in range(sentence_length):\n",
        "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
        "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
        "        x = np.zeros((X_size, 1))\n",
        "        x[idx] = 1\n",
        "        indexes.append(idx)\n",
        "\n",
        "    return indexes"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUPQpEimO_aK"
      },
      "source": [
        "def update_status(inputs, h_prev, C_prev):\n",
        "    #initialized later\n",
        "    global plot_iter, plot_loss\n",
        "    global smooth_loss\n",
        "    \n",
        "    # Get predictions for 200 letters with current model\n",
        "\n",
        "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
        "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
        "\n",
        "    # Clear and plot\n",
        "    plt.plot(plot_iter, plot_loss)\n",
        "    display.clear_output(wait=True)\n",
        "    plt.show()\n",
        "\n",
        "    #Print prediction and loss\n",
        "    print(\"----\\n %s \\n----\" % (txt, ))\n",
        "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ho1YKN0O_aL"
      },
      "source": [
        "def update_paramters(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.m += p.d * p.d # Calculate sum of gradients\n",
        "        #print(learning_rate * dparam)\n",
        "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVHGY4wUOzHr"
      },
      "source": [
        ""
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH7ikVbNO3Gg"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQyNSL0iJOxH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "52146497-f8c9-4faf-9f1d-99b97b655b98"
      },
      "source": [
        "iter = 50_000\n",
        "while iter > 0:\n",
        "  # Reset\n",
        "  if pointer + Time_steps >= len(data) or iteration == 0:\n",
        "      g_h_prev = np.zeros((Hidden_Layer_size, 1))\n",
        "      g_C_prev = np.zeros((Hidden_Layer_size, 1))\n",
        "      pointer = 0\n",
        "\n",
        "\n",
        "  inputs = ([char_to_idx[ch] \n",
        "              for ch in data[pointer: pointer + Time_steps]])\n",
        "  targets = ([char_to_idx[ch] \n",
        "              for ch in data[pointer + 1: pointer + Time_steps + 1]])\n",
        "\n",
        "  loss, g_h_prev, g_C_prev = \\\n",
        "      forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "  # Print every hundred steps\n",
        "  if iteration % 100 == 0:\n",
        "      update_status(inputs, g_h_prev, g_C_prev)\n",
        "\n",
        "  update_paramters()\n",
        "\n",
        "  plot_iter = np.append(plot_iter, [iteration])\n",
        "  plot_loss = np.append(plot_loss, [loss])\n",
        "\n",
        "  pointer += Time_steps\n",
        "  iteration += 1\n",
        "  iter = iter -1"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " sprealth restals, and include Thailano dastacingar. In id to to sperime flingiry in other countries, which ary any tiang treat reportlt of thein werkention hive evacing as cnerler, but not late. Soath \n",
            "----\n",
            "iter 49900, loss 4.766710\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AKpa1BGOItQ"
      },
      "source": [
        "# Quiz Question 8\n",
        "\n",
        "Run the above code for 50000 iterations making sure that you have 100 hidden layers and time_steps is 40. What is the loss value you're seeing?"
      ]
    }
  ]
}